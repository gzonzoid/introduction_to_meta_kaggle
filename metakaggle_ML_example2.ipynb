{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Kaggle ML example\n",
    "\n",
    "Goal: predict user ranking from\n",
    " - first and last submission date\n",
    " - team leader tier (experience level)\n",
    "\n",
    "Features to add:\n",
    " - number of submissions made to the contest\n",
    " - number of contests participated in (by leader or whole team?)\n",
    "\n",
    "Other things to add:\n",
    " - improve prediction: instead of raw ranking normalize by total number after dropping inactives (single submissions?), try predicting medaled or not (too imbalanced?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_day(dates):\n",
    "    \"\"\" calculate a numeric value for a pandas series of dates\"\"\"\n",
    "    dayofyear = dates.dt.dayofyear\n",
    "    year = dates.dt.year\n",
    "\n",
    "    # subtract off the first year and calculate the days\n",
    "    year = year - min(year)\n",
    "    day = dayofyear + year * 365\n",
    "    return day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_drop(feature, predict):\n",
    "    # combine dataframes\n",
    "    tmp_features = pd.concat([feature, predict], axis=1)\n",
    "    # drop missing values\n",
    "    tmp_features = tmp_features.dropna(how='any')\n",
    "    # drop duplicate rows\n",
    "    tmp_features = tmp_features.drop_duplicates()\n",
    "    # reseparate features from prediction values\n",
    "    final_predict = tmp_features.iloc[:,-1]\n",
    "    final_feature = tmp_features.iloc[:,0:-1]\n",
    "    return final_feature, final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scores_to_teams(submissions, teams, verbose=False):\n",
    "    # add a joining column to the submissions matrix\n",
    "    submissions['join_teams_submissions'] = submissions.index\n",
    "    submissions['PublicLeaderboardScore'] = submissions.PublicScoreFullPrecision\n",
    "    submissions['PrivateLeaderboardScore'] = submissions.PrivateScoreFullPrecision\n",
    "    # extract the column to add (and keep column created for the join)\n",
    "    public_leaderboard = submissions[['join_teams_submissions', 'PublicLeaderboardScore']]\n",
    "    private_leaderboard = submissions[['join_teams_submissions', 'PrivateLeaderboardScore']]\n",
    "    # create a column with the same name in the teams matrix\n",
    "    teams['join_teams_submissions'] = teams.PublicLeaderboardSubmissionId\n",
    "    # perform the merge\n",
    "    teams_with_score = teams.join(public_leaderboard, on='join_teams_submissions', rsuffix='_j1')\n",
    "    teams_with_score = teams_with_score.join(private_leaderboard, on='join_teams_submissions', rsuffix='_j2')\n",
    "    if verbose:\n",
    "        print('shape of the team, public_leaderboard, and new teams_with_score data frames:')\n",
    "        print(teams.shape, public_leaderboard.shape, teams_with_score.shape)\n",
    "    # drop the columns added for joining\n",
    "    drop_cols = ['join_teams_submissions_j1', 'join_teams_submissions_j2', 'join_teams_submissions']\n",
    "    teams_with_score = teams_with_score.drop(drop_cols, axis=1)\n",
    "    if verbose:\n",
    "        print('shape of teams_with_score matrix after dropping redundant columns: ')\n",
    "        print(teams_with_score.shape)\n",
    "    return teams_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_leader_tier(users, teams, verbose=False):\n",
    "    # add a joining column \n",
    "    users['join'] = users.index\n",
    "    # create a column with the same name in the teams matrix\n",
    "    teams['join'] = teams.TeamLeaderId\n",
    "    \n",
    "    # perform the merge\n",
    "    teams_with_tier = teams.join(users, on='join', rsuffix='_j1')\n",
    "    if verbose:\n",
    "        print('shape of the team, public_leaderboard, and new teams_with_score data frames:')\n",
    "        print(teams.shape, users.shape, teams_with_tier.shape)\n",
    "    # drop the columns added for joining\n",
    "    drop_cols = ['join_j1', 'join', 'UserName', 'DisplayName']\n",
    "    teams_with_tier = teams_with_tier.drop(drop_cols, axis=1)\n",
    "    if verbose:\n",
    "        print('shape of teams_with_score matrix after dropping redundant columns: ')\n",
    "        print(teams_with_tier.shape)\n",
    "        print('columns in final dataframe: ')\n",
    "        print(teams_with_tier.columns.values)\n",
    "    return teams_with_tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sklearn.ensemble as ske\n",
    "import meta_kaggle_utils as utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'meta-kaggle/'\n",
    "submission_file_name = 'Submissions.csv'\n",
    "team_file_name = 'Teams.csv'\n",
    "users_file = 'Users.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 2919592 rows.\n",
      "The table contains the following columns: \n",
      "['UserName' 'DisplayName' 'RegisterDate' 'PerformanceTier']\n"
     ]
    }
   ],
   "source": [
    "users = utils.load_kaggle_csv(data_location + users_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (5,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 5015235 rows.\n",
      "The table contains the following columns: \n",
      "['SubmittedUserId' 'TeamId' 'SourceKernelVersionId' 'SubmissionDate'\n",
      " 'ScoreDate' 'IsAfterDeadline' 'PublicScoreLeaderboardDisplay'\n",
      " 'PublicScoreFullPrecision' 'PrivateScoreLeaderboardDisplay'\n",
      " 'PrivateScoreFullPrecision']\n"
     ]
    }
   ],
   "source": [
    "# load the submission file\n",
    "submissions = utils.load_kaggle_csv(data_location + submission_file_name)\n",
    "# convert scores to numeric values\n",
    "submissions['PublicScoreFullPrecision'] = pd.to_numeric(submissions['PublicScoreFullPrecision'], errors='coerce')\n",
    "submissions['PrivateScoreFullPrecision'] = pd.to_numeric(submissions['PrivateScoreFullPrecision'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 1740398 rows.\n",
      "The table contains the following columns: \n",
      "['CompetitionId' 'TeamLeaderId' 'TeamName' 'ScoreFirstSubmittedDate'\n",
      " 'LastSubmissionDate' 'PublicLeaderboardSubmissionId'\n",
      " 'PrivateLeaderboardSubmissionId' 'IsBenchmark' 'Medal' 'MedalAwardDate'\n",
      " 'PublicLeaderboardRank' 'PrivateLeaderboardRank']\n"
     ]
    }
   ],
   "source": [
    "# load the teams file\n",
    "teams = utils.load_kaggle_csv(data_location + team_file_name)\n",
    "# drop teams that never submitted anything\n",
    "teams = teams.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of dates before and after conversion to timestamps: \n",
      "<class 'str'>\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "# convert dates to datetime objects\n",
    "print('type of dates before and after conversion to timestamps: ')\n",
    "print(type(teams.ScoreFirstSubmittedDate[497]))\n",
    "teams.ScoreFirstSubmittedDate = pd.to_datetime(teams.ScoreFirstSubmittedDate)\n",
    "print(type(teams.ScoreFirstSubmittedDate[497]))\n",
    "\n",
    "# repeate with last date\n",
    "teams.LastSubmissionDate = pd.to_datetime(teams.LastSubmissionDate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates as timestamps:\n",
      "Id\n",
      "497   2010-04-30\n",
      "500   2010-05-02\n",
      "503   2010-05-05\n",
      "504   2010-05-11\n",
      "505   2010-05-19\n",
      "Name: ScoreFirstSubmittedDate, dtype: datetime64[ns]\n",
      "dates as numbers:\n",
      "Id\n",
      "497    120\n",
      "500    122\n",
      "503    125\n",
      "504    131\n",
      "505    139\n",
      "Name: ScoreFirstSubmittedDate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert date to a number of days after Jan 1st of the first year of data in the dataset\n",
    "days = date_to_day(teams.ScoreFirstSubmittedDate)\n",
    "teams['first_date_as_day'] = days\n",
    "\n",
    "print('dates as timestamps:')\n",
    "print(teams.ScoreFirstSubmittedDate[0:5])\n",
    "print('dates as numbers:')\n",
    "print(days[0:5])\n",
    "\n",
    "# repeate for the last date\n",
    "teams['last_date_as_day'] = date_to_day(teams.LastSubmissionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teams matrix shape:  (22234, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompetitionId</th>\n",
       "      <th>TeamLeaderId</th>\n",
       "      <th>TeamName</th>\n",
       "      <th>ScoreFirstSubmittedDate</th>\n",
       "      <th>LastSubmissionDate</th>\n",
       "      <th>PublicLeaderboardSubmissionId</th>\n",
       "      <th>PrivateLeaderboardSubmissionId</th>\n",
       "      <th>IsBenchmark</th>\n",
       "      <th>Medal</th>\n",
       "      <th>MedalAwardDate</th>\n",
       "      <th>PublicLeaderboardRank</th>\n",
       "      <th>PrivateLeaderboardRank</th>\n",
       "      <th>first_date_as_day</th>\n",
       "      <th>last_date_as_day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2435</td>\n",
       "      <td>619.0</td>\n",
       "      <td>jonp</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>2182.0</td>\n",
       "      <td>2182.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>07/15/2016</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2435</td>\n",
       "      <td>673.0</td>\n",
       "      <td>Thylacoleo</td>\n",
       "      <td>2010-05-02</td>\n",
       "      <td>2010-07-10</td>\n",
       "      <td>2187.0</td>\n",
       "      <td>2187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>07/15/2016</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>122</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2435</td>\n",
       "      <td>672.0</td>\n",
       "      <td>Fontanelles</td>\n",
       "      <td>2010-05-05</td>\n",
       "      <td>2010-05-08</td>\n",
       "      <td>2199.0</td>\n",
       "      <td>2199.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>07/15/2016</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>125</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2435</td>\n",
       "      <td>727.0</td>\n",
       "      <td>IFM_bioinformatics</td>\n",
       "      <td>2010-05-11</td>\n",
       "      <td>2010-05-12</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>2246.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>07/15/2016</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2435</td>\n",
       "      <td>728.0</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2010-05-19</td>\n",
       "      <td>2010-05-19</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>2308.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>07/15/2016</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>139</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CompetitionId  TeamLeaderId            TeamName ScoreFirstSubmittedDate  \\\n",
       "Id                                                                             \n",
       "497           2435         619.0                jonp              2010-04-30   \n",
       "500           2435         673.0          Thylacoleo              2010-05-02   \n",
       "503           2435         672.0         Fontanelles              2010-05-05   \n",
       "504           2435         727.0  IFM_bioinformatics              2010-05-11   \n",
       "505           2435         728.0           Amsterdam              2010-05-19   \n",
       "\n",
       "    LastSubmissionDate  PublicLeaderboardSubmissionId  \\\n",
       "Id                                                      \n",
       "497         2010-04-30                         2182.0   \n",
       "500         2010-07-10                         2187.0   \n",
       "503         2010-05-08                         2199.0   \n",
       "504         2010-05-12                         2203.0   \n",
       "505         2010-05-19                         2306.0   \n",
       "\n",
       "     PrivateLeaderboardSubmissionId  IsBenchmark  Medal MedalAwardDate  \\\n",
       "Id                                                                       \n",
       "497                          2182.0        False    3.0     07/15/2016   \n",
       "500                          2187.0        False    3.0     07/15/2016   \n",
       "503                          2199.0        False    3.0     07/15/2016   \n",
       "504                          2246.0        False    1.0     07/15/2016   \n",
       "505                          2308.0        False    2.0     07/15/2016   \n",
       "\n",
       "     PublicLeaderboardRank  PrivateLeaderboardRank  first_date_as_day  \\\n",
       "Id                                                                      \n",
       "497                   41.0                    25.0                120   \n",
       "500                   31.0                    23.0                122   \n",
       "503                    6.0                    31.0                125   \n",
       "504                   13.0                     9.0                131   \n",
       "505                   18.0                    11.0                139   \n",
       "\n",
       "     last_date_as_day  \n",
       "Id                     \n",
       "497               120  \n",
       "500               191  \n",
       "503               128  \n",
       "504               132  \n",
       "505               139  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('teams matrix shape: ', teams.shape)\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to teams matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the team, public_leaderboard, and new teams_with_score data frames:\n",
      "(22234, 15) (5015235, 2) (22234, 19)\n",
      "shape of teams_with_score matrix after dropping redundant columns: \n",
      "(22234, 16)\n"
     ]
    }
   ],
   "source": [
    "teams_with_score = add_scores_to_teams(submissions, teams, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the team, public_leaderboard, and new teams_with_score data frames:\n",
      "(22234, 17) (2919592, 5) (22234, 22)\n",
      "shape of teams_with_score matrix after dropping redundant columns: \n",
      "(22234, 18)\n",
      "columns in final dataframe: \n",
      "['CompetitionId' 'TeamLeaderId' 'TeamName' 'ScoreFirstSubmittedDate'\n",
      " 'LastSubmissionDate' 'PublicLeaderboardSubmissionId'\n",
      " 'PrivateLeaderboardSubmissionId' 'IsBenchmark' 'Medal' 'MedalAwardDate'\n",
      " 'PublicLeaderboardRank' 'PrivateLeaderboardRank' 'first_date_as_day'\n",
      " 'last_date_as_day' 'PublicLeaderboardScore' 'PrivateLeaderboardScore'\n",
      " 'RegisterDate' 'PerformanceTier']\n"
     ]
    }
   ],
   "source": [
    "teams_with_tier = add_leader_tier(users, teams_with_score, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up the matrix for the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns available for use: \n",
      "['CompetitionId' 'TeamLeaderId' 'TeamName' 'ScoreFirstSubmittedDate'\n",
      " 'LastSubmissionDate' 'PublicLeaderboardSubmissionId'\n",
      " 'PrivateLeaderboardSubmissionId' 'IsBenchmark' 'Medal' 'MedalAwardDate'\n",
      " 'PublicLeaderboardRank' 'PrivateLeaderboardRank' 'first_date_as_day'\n",
      " 'last_date_as_day' 'PublicLeaderboardScore' 'PrivateLeaderboardScore'\n",
      " 'RegisterDate' 'PerformanceTier']\n"
     ]
    }
   ],
   "source": [
    "print('columns available for use: ')\n",
    "print(teams_with_tier.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected feature column: PrivateLeaderboardRank\n"
     ]
    }
   ],
   "source": [
    "# select feature column. Must be in the list above\n",
    "predict_col = 'PrivateLeaderboardRank'\n",
    "print('selected feature column: ' + predict_col)\n",
    "\n",
    "# select fetture columns to use. These must be selected from the list above\n",
    "feature_cols = ['CompetitionId', 'first_date_as_day', 'last_date_as_day', 'PerformanceTier']\n",
    "\n",
    "# actually select the things\n",
    "feature_matrix = teams_with_tier[feature_cols]\n",
    "prediction = teams_with_tier[predict_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape before and after droping missing values\n",
      "(22234, 4) (22234,)\n",
      "(21566, 4) (21566,)\n"
     ]
    }
   ],
   "source": [
    "# drop missing values & duplicates\n",
    "print('feature matrix shape before and after droping missing values')\n",
    "# If it gets much smaller, something is not working well. You might need to impute \n",
    "#   missing values (or look for a bug in your code)\n",
    "print(feature_matrix.shape, prediction.shape)\n",
    "feature_matrix, prediction = ml_drop(feature_matrix, prediction)\n",
    "print(feature_matrix.shape, prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of feature matrix:\n",
      "     CompetitionId  first_date_as_day  last_date_as_day  PerformanceTier\n",
      "Id                                                                      \n",
      "504           2435                131               132              2.0\n",
      "505           2435                139               139              1.0\n",
      "508           2435                127               127              2.0\n",
      "509           2435                214               214              1.0\n",
      "510           2435                134               135              2.0\n",
      "start of prediction matrix\n",
      "Id\n",
      "504     9.0\n",
      "505    11.0\n",
      "508     3.0\n",
      "509     4.0\n",
      "510    35.0\n",
      "Name: PrivateLeaderboardRank, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('start of feature matrix:')\n",
    "print(feature_matrix.head())\n",
    "print('start of prediction matrix')\n",
    "print(prediction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21566, 4) shape of feature matrix\n",
      "21566 length of predictions\n",
      "Two of the numbers above should be the same.\n"
     ]
    }
   ],
   "source": [
    "# make sure things are the right shapes\n",
    "print(feature_matrix.shape, 'shape of feature matrix')\n",
    "print(len(prediction), 'length of predictions')\n",
    "print('Two of the numbers above should be the same.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "           oob_score=True, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a random forest regression\n",
    "regr = ske.RandomForestRegressor(max_depth=2, random_state=0,\n",
    "                             n_estimators=100, oob_score=True)\n",
    "regr.fit(feature_matrix, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22647569993703032 oob score\n",
      "features and their importance\n",
      "['CompetitionId' 'first_date_as_day' 'last_date_as_day' 'PerformanceTier']\n",
      "[0.80943281 0.         0.         0.19056719]\n",
      "prediction for [2435, 130, 132, 1]\n",
      "[46.33826152]\n",
      "prediction for [4495, 130, 132, 1]\n",
      "[81.23282303]\n",
      "prediction for [4495, 130, 132, 5]\n",
      "[81.23282303]\n"
     ]
    }
   ],
   "source": [
    "print(regr.oob_score_, 'oob score')\n",
    "\n",
    "print('features and their importance')\n",
    "print(feature_matrix.columns.values)\n",
    "print(regr.feature_importances_)\n",
    "\n",
    "a = [2435, 130, 132, 1]\n",
    "print('prediction for', a)\n",
    "print(regr.predict([a]))\n",
    "a = [4495, 130, 132, 1]\n",
    "print('prediction for', a)\n",
    "print(regr.predict([a]))\n",
    "a = [4495, 130, 132, 5]\n",
    "print('prediction for', a)\n",
    "print(regr.predict([a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CompetitionId  first_date_as_day  last_date_as_day  PerformanceTier\n",
      "Id                                                                      \n",
      "504           2435                131               132              2.0\n",
      "505           2435                139               139              1.0\n",
      "508           2435                127               127              2.0\n",
      "509           2435                214               214              1.0\n",
      "510           2435                134               135              2.0\n",
      "512           2435                130               130              3.0\n",
      "513           2435                153               153              2.0\n",
      "522           2435                137               137              1.0\n",
      "552           2435                186               186              3.0\n",
      "596           2435                153               153              3.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# explore reasonable values for making predictions\n",
    "print(feature_matrix.iloc[0:10])\n",
    "print(max(feature_matrix.PerformanceTier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for a competition where group size matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model for the single largest competition\n",
    "It should be an easier problem if all of the data is from the same competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common competition\n",
    "num_occur = feature_matrix.CompetitionId.value_counts()\n",
    "print(num_occur.iloc[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_use = num_occur.index.values[0]\n",
    "competition_use = 8076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data from that commetition\n",
    "features_competition = feature_matrix[feature_matrix.CompetitionId == competition_use]\n",
    "features_competition = features_competition.drop('CompetitionId', axis=1)\n",
    "predict_competition = prediction[feature_matrix.CompetitionId == competition_use]\n",
    "\n",
    "# make sure things are the right shapes\n",
    "print(features_competition.shape, 'shape of feature matrix')\n",
    "print(len(predict_competition), 'length of predictions')\n",
    "print('Two of the numbers above should be the same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest regression\n",
    "regr2 = ske.RandomForestRegressor(max_depth=2, random_state=0,\n",
    "                             n_estimators=100, oob_score=True)\n",
    "regr2.fit(features_competition, predict_competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the results\n",
    "print(regr2.oob_score_, 'oob score')\n",
    "\n",
    "print('features and their importance')\n",
    "print(features_competition.columns.values)\n",
    "print(regr2.feature_importances_)\n",
    "\n",
    "a = [2888, 2889]\n",
    "print('prediction for', a)\n",
    "print(regr2.predict([a]))\n",
    "a = [2888, 2905]\n",
    "print('prediction for', a)\n",
    "print(regr2.predict([a]))\n",
    "a = [2950, 2982]\n",
    "print('prediction for', a)\n",
    "print(regr2.predict([a]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to figure out what reasonable values are for new predictions\n",
    "print(features_competition.iloc[0:10])\n",
    "print(features_competition.iloc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features\n",
    "I would like to add \n",
    "- the number of submissions\n",
    "- the number of teammembers\n",
    "- length of team name\n",
    "- number of contests participated in (by leader or whole team?)\n",
    "\n",
    "Available columns:\n",
    "- submissions: 'SubmittedUserId', 'TeamId', 'SourceKernelVersionId','SubmissionDate', 'ScoreDate', 'IsAfterDeadline', 'PublicScoreLeaderboardDisplay', 'PublicScoreFullPrecision','PrivateScoreLeaderboardDisplay', 'PrivateScoreFullPrecision', 'PublicLeaderboardScore','PrivateLeaderboardScore'\n",
    "- team: 'CompetitionId', 'TeamLeaderId', 'TeamName', 'ScoreFirstSubmittedDate', 'LastSubmissionDate', 'PublicLeaderboardSubmissionId', 'PrivateLeaderboardSubmissionId', 'IsBenchmark', 'Medal', 'MedalAwardDate', 'PublicLeaderboardRank', 'PrivateLeaderboardRank'\n",
    "- TeamMemberships: Id, TeamId, UserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get team leader rank (PerformanceTier)\n",
    "#   - load Users\n",
    "#   - join with teams_with_scores on TeamLeaderId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
