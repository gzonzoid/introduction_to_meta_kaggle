{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Kaggle ML example\n",
    "Goal: predict user ranking from\n",
    " - first and last submission date\n",
    "To add:\n",
    " - number of submissions made to the contest\n",
    " - number of team members\n",
    " - number of contests participated in (by leader or whole team?)\n",
    " - improve prediction: instead of raw ranking normalize by total number after dropping inactives (single submissions?), try predicting medaled or not (too imbalanced?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import meta_kaggle_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'meta-kaggle/'\n",
    "submission_file_name = 'Submissions.csv'\n",
    "team_file_name = 'Teams.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load submissions and teams files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (5,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 5015235 rows.\n",
      "The table contains the following columns: \n",
      "['SubmittedUserId' 'TeamId' 'SourceKernelVersionId' 'SubmissionDate'\n",
      " 'ScoreDate' 'IsAfterDeadline' 'PublicScoreLeaderboardDisplay'\n",
      " 'PublicScoreFullPrecision' 'PrivateScoreLeaderboardDisplay'\n",
      " 'PrivateScoreFullPrecision']\n"
     ]
    }
   ],
   "source": [
    "submissions = utils.load_kaggle_csv(data_location + submission_file_name)\n",
    "submissions['PublicScoreFullPrecision'] = pd.to_numeric(submissions['PublicScoreFullPrecision'], errors='coerce')\n",
    "submissions['PrivateScoreFullPrecision'] = pd.to_numeric(submissions['PrivateScoreFullPrecision'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 1740398 rows.\n",
      "The table contains the following columns: \n",
      "['CompetitionId' 'TeamLeaderId' 'TeamName' 'ScoreFirstSubmittedDate'\n",
      " 'LastSubmissionDate' 'PublicLeaderboardSubmissionId'\n",
      " 'PrivateLeaderboardSubmissionId' 'IsBenchmark' 'Medal' 'MedalAwardDate'\n",
      " 'PublicLeaderboardRank' 'PrivateLeaderboardRank']\n"
     ]
    }
   ],
   "source": [
    "teams = utils.load_kaggle_csv(data_location + team_file_name)\n",
    "teams = teams.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge submission and teams files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5015235, 10)\n",
      "(5015235, 13)\n"
     ]
    }
   ],
   "source": [
    "# add a joining column to the submissions matrix\n",
    "submissions['join_teams_submissions'] = submissions.index\n",
    "submissions['PublicLeaderboardScore'] = submissions.PublicScoreFullPrecision\n",
    "submissions['PrivateLeaderboardScore'] = submissions.PrivateScoreFullPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5015235, 2)\n"
     ]
    }
   ],
   "source": [
    "# extract the column to add (and keep the joining column)\n",
    "public_leaderboard = submissions[['join_teams_submissions', 'PublicLeaderboardScore']]\n",
    "private_leaderboard = submissions[['join_teams_submissions', 'PrivateLeaderboardScore']]\n",
    "print(public_leaderboard.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22234, 13)\n"
     ]
    }
   ],
   "source": [
    "teams['join_teams_submissions'] = teams.PublicLeaderboardSubmissionId\n",
    "print(teams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22234, 13) (5015235, 2) (22234, 17)\n",
      "(22234, 14)\n"
     ]
    }
   ],
   "source": [
    "# perform the merge\n",
    "teams_with_score = teams.join(public_leaderboard, on='join_teams_submissions', rsuffix='_j1')\n",
    "teams_with_score = teams_with_score.join(private_leaderboard, on='join_teams_submissions', rsuffix='_j2')\n",
    "print(teams.shape, public_leaderboard.shape, teams_with_score.shape)\n",
    "\n",
    "# drop the columns added for joining\n",
    "drop_cols = ['join_teams_submissions_j1', 'join_teams_submissions_j2', 'join_teams_submissions']\n",
    "teams_with_score = teams_with_score.drop(drop_cols, axis=1)\n",
    "print(teams_with_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up the matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
