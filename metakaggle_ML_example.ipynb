{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Kaggle ML example\n",
    "\n",
    "Goal - predict user ranking from:\n",
    " - first and last submission date\n",
    " - team leader tier (experience level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_day(dates):\n",
    "    \"\"\" calculate a numeric value for a pandas series of dates\"\"\"\n",
    "    dayofyear = dates.dt.dayofyear\n",
    "    year = dates.dt.year\n",
    "\n",
    "    # subtract off the first year and calculate the days\n",
    "    year = year - min(year)\n",
    "    day = dayofyear + year * 365\n",
    "    return day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_drop(feature, predict):\n",
    "    \"\"\" \n",
    "    Drop columns with missing values from the feature dataframe. \n",
    "    Keeps the predict series aligned by dropping the same entries.\n",
    "    \"\"\"\n",
    "    # combine dataframes\n",
    "    tmp_features = pd.concat([feature, predict], axis=1)\n",
    "    # drop missing values\n",
    "    tmp_features = tmp_features.dropna(how='any')\n",
    "    # drop duplicate rows\n",
    "    tmp_features = tmp_features.drop_duplicates()\n",
    "    # reseparate features from prediction values\n",
    "    final_predict = tmp_features.iloc[:,-1]\n",
    "    final_feature = tmp_features.iloc[:,0:-1]\n",
    "    return final_feature, final_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scores_to_teams(submissions, teams, verbose=False):\n",
    "    \"\"\" \n",
    "    Adds the public and private leaderboard scores to the teams matrix. Assumes \n",
    "    the columns have the names provided in the meta-kaggle csv files.\n",
    "    \"\"\"\n",
    "    # add a joining column to the submissions matrix\n",
    "    submissions['join_teams_submissions'] = submissions.index\n",
    "    submissions['PublicLeaderboardScore'] = submissions.PublicScoreFullPrecision\n",
    "    submissions['PrivateLeaderboardScore'] = submissions.PrivateScoreFullPrecision\n",
    "    # extract the column to add (and keep column created for the join)\n",
    "    public_leaderboard = submissions[['join_teams_submissions', 'PublicLeaderboardScore']]\n",
    "    private_leaderboard = submissions[['join_teams_submissions', 'PrivateLeaderboardScore']]\n",
    "    # create a column with the same name in the teams matrix\n",
    "    teams['join_teams_submissions'] = teams.PublicLeaderboardSubmissionId\n",
    "    # perform the merge\n",
    "    teams_with_score = teams.join(public_leaderboard, on='join_teams_submissions', rsuffix='_j1')\n",
    "    teams_with_score = teams_with_score.join(private_leaderboard, on='join_teams_submissions', rsuffix='_j2')\n",
    "    if verbose:\n",
    "        print('shape of the team, public_leaderboard, and new teams_with_score data frames:')\n",
    "        print(teams.shape, public_leaderboard.shape, teams_with_score.shape)\n",
    "    # drop the columns added for joining\n",
    "    drop_cols = ['join_teams_submissions_j1', 'join_teams_submissions_j2', 'join_teams_submissions']\n",
    "    teams_with_score = teams_with_score.drop(drop_cols, axis=1)\n",
    "    if verbose:\n",
    "        print('shape of teams_with_score matrix after dropping redundant columns: ')\n",
    "        print(teams_with_score.shape)\n",
    "    return teams_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_leader_tier(users, teams, verbose=False):\n",
    "    \"\"\" \n",
    "    Adds the tier (measure of kaggle experience) of the leader to the teams matrix. \n",
    "    Assumes the columns have the names provided in the meta-kaggle csv files.\n",
    "    \"\"\"\n",
    "    # add a joining column \n",
    "    users['join'] = users.index\n",
    "    # create a column with the same name in the teams matrix\n",
    "    teams['join'] = teams.TeamLeaderId\n",
    "    \n",
    "    # perform the merge\n",
    "    teams_with_tier = teams.join(users, on='join', rsuffix='_j1')\n",
    "    if verbose:\n",
    "        print('shape of the team, public_leaderboard, and new teams_with_score data frames:')\n",
    "        print(teams.shape, users.shape, teams_with_tier.shape)\n",
    "    # drop the columns added for joining\n",
    "    drop_cols = ['join_j1', 'join', 'UserName', 'DisplayName']\n",
    "    teams_with_tier = teams_with_tier.drop(drop_cols, axis=1)\n",
    "    if verbose:\n",
    "        print('shape of teams_with_score matrix after dropping redundant columns: ')\n",
    "        print(teams_with_tier.shape)\n",
    "        print('columns in final dataframe: ')\n",
    "        print(teams_with_tier.columns.values)\n",
    "    return teams_with_tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sklearn.ensemble as ske\n",
    "import meta_kaggle_utils as utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up file paths\n",
    "If you unziped the meta-kaggle data file in a different folder, change data_location below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'meta-kaggle/'\n",
    "submission_file_name = 'Submissions.csv'\n",
    "team_file_name = 'Teams.csv'\n",
    "users_file = 'Users.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 2919592 rows.\n",
      "The table contains the following columns: \n",
      "['UserName' 'DisplayName' 'RegisterDate' 'PerformanceTier']\n"
     ]
    }
   ],
   "source": [
    "# load the users\n",
    "users = utils.load_kaggle_csv(data_location + users_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (5,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# load the submission file\n",
    "submissions = utils.load_kaggle_csv(data_location + submission_file_name)\n",
    "# convert scores to numeric values\n",
    "submissions['PublicScoreFullPrecision'] = pd.to_numeric(submissions['PublicScoreFullPrecision'], errors='coerce')\n",
    "submissions['PrivateScoreFullPrecision'] = pd.to_numeric(submissions['PrivateScoreFullPrecision'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the teams file\n",
    "teams = utils.load_kaggle_csv(data_location + team_file_name)\n",
    "# drop teams that never submitted anything\n",
    "teams = teams.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to datetime objects\n",
    "print('type of dates before and after conversion to timestamps: ')\n",
    "print(type(teams.ScoreFirstSubmittedDate[497]))\n",
    "teams.ScoreFirstSubmittedDate = pd.to_datetime(teams.ScoreFirstSubmittedDate)\n",
    "print(type(teams.ScoreFirstSubmittedDate[497]))\n",
    "# repeate with last date\n",
    "teams.LastSubmissionDate = pd.to_datetime(teams.LastSubmissionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to a number of days after Jan 1st of the first year of data in the dataset\n",
    "days = date_to_day(teams.ScoreFirstSubmittedDate)\n",
    "teams['first_date_as_day'] = days\n",
    "\n",
    "print('dates as timestamps:')\n",
    "print(teams.ScoreFirstSubmittedDate[0:5])\n",
    "print('dates as numbers:')\n",
    "print(days[0:5])\n",
    "\n",
    "# repeate for the last date\n",
    "teams['last_date_as_day'] = date_to_day(teams.LastSubmissionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('teams matrix shape: ', teams.shape)\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to teams matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_with_score = add_scores_to_teams(submissions, teams, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_with_tier = add_leader_tier(users, teams_with_score, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up the matrix for the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('columns available for use: ')\n",
    "print(teams_with_tier.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature column. Must be in the list above\n",
    "predict_col = 'PrivateLeaderboardRank'\n",
    "print('selected feature column: ' + predict_col)\n",
    "\n",
    "# select fetture columns to use. These must be selected from the list above\n",
    "feature_cols = ['CompetitionId', 'first_date_as_day', 'last_date_as_day', 'PerformanceTier']\n",
    "\n",
    "# actually select the things\n",
    "feature_matrix = teams_with_tier[feature_cols]\n",
    "prediction = teams_with_tier[predict_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values & duplicates\n",
    "print('feature matrix shape before and after droping missing values')\n",
    "# If it gets much smaller, something is not working well. You might need to impute \n",
    "#   missing values (or look for a bug in your code)\n",
    "print(feature_matrix.shape, prediction.shape)\n",
    "feature_matrix, prediction = ml_drop(feature_matrix, prediction)\n",
    "print(feature_matrix.shape, prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start of feature matrix:')\n",
    "print(feature_matrix.head())\n",
    "print('start of prediction matrix')\n",
    "print(prediction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure things are the right shapes\n",
    "print(feature_matrix.shape, 'shape of feature matrix')\n",
    "print(len(prediction), 'length of predictions')\n",
    "print('Two of the numbers above should be the same.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest regression\n",
    "regr = ske.RandomForestRegressor(max_depth=2, random_state=0,\n",
    "                             n_estimators=100, oob_score=True)\n",
    "regr.fit(feature_matrix, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr.oob_score_, 'oob score')\n",
    "print()\n",
    "print('features and their importance')\n",
    "print(feature_matrix.columns.values)\n",
    "print(regr.feature_importances_)\n",
    "print()\n",
    "\n",
    "a = [2435, 130, 132, 1]\n",
    "print('prediction for', a)\n",
    "print(regr.predict([a]))\n",
    "a = [4495, 130, 132, 1]\n",
    "print('prediction for', a)\n",
    "print(regr.predict([a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model for the single largest competition\n",
    "It should be an easier problem if all of the data is from the same competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common competition\n",
    "num_occur = feature_matrix.CompetitionId.value_counts()\n",
    "print(num_occur.iloc[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_use = num_occur.index.values[0]\n",
    "competition_use = 8076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data from that commetition\n",
    "features_competition = feature_matrix[feature_matrix.CompetitionId == competition_use]\n",
    "features_competition = features_competition.drop('CompetitionId', axis=1)\n",
    "predict_competition = prediction[feature_matrix.CompetitionId == competition_use]\n",
    "\n",
    "# make sure things are the right shapes\n",
    "print(features_competition.shape, 'shape of feature matrix')\n",
    "print(len(predict_competition), 'length of predictions')\n",
    "print('Two of the numbers above should be the same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest regression\n",
    "regr2 = ske.RandomForestRegressor(max_depth=2, random_state=0,\n",
    "                             n_estimators=100, oob_score=True)\n",
    "regr2.fit(features_competition, predict_competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the results\n",
    "print(regr2.oob_score_, 'oob score')\n",
    "\n",
    "print('features and their importance')\n",
    "print(features_competition.columns.values)\n",
    "print(regr2.feature_importances_)\n",
    "\n",
    "a = [2888, 2889, 1]\n",
    "print('prediction for', a)\n",
    "print(regr2.predict([a]))\n",
    "a = [2888, 2889, 5]\n",
    "print('prediction for', a)\n",
    "print(regr2.predict([a]))\n",
    "a = [2950, 2982, 1]\n",
    "print('prediction for', a)\n",
    "print(regr2.predict([a]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to figure out what reasonable values are for new predictions\n",
    "print(features_competition.iloc[0:10])\n",
    "print(features_competition.iloc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features\n",
    "\n",
    "Features that could be added:\n",
    "- the number of submissions\n",
    "- the number of team members\n",
    "- number of contests participated in\n",
    "- number of forum posts\n",
    "- number of kernels published\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
